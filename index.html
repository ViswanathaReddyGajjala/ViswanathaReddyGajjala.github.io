<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Viswanatha Reddy</title>
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <!-- Import Modern Font -->
  <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600&display=swap" rel="stylesheet">
  <style>
    /* CSS Variables for easy theming */
    :root {
      --primary-color: #1e1e1e;
      --secondary-color: #f4f4f4;
      --accent-color: #0077cc;
      --bg-gradient: linear-gradient(135deg, #f8f9fa, #e9ecef);
      --font-family: 'Poppins', sans-serif;
    }
    
    /* CSS Reset */
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }
    
    body {
      font-family: var(--font-family);
      background: var(--bg-gradient);
      color: var(--primary-color);
      line-height: 1.6;
      overflow-x: hidden;
    }
    
    a {
      color: var(--accent-color);
      text-decoration: none;
      transition: color 0.3s ease;
    }
    
    a:hover {
      text-decoration: underline;
    }
    
    .container {
      width: 90%;
      max-width: 1200px;
      margin: auto;
      padding: 0 1rem;
    }
    
    /* Header & Navigation */
    header {
      background: rgba(30, 30, 30, 0.9);
      color: #fff;
      padding: 1rem 0;
      position: sticky;
      top: 0;
      z-index: 100;
      backdrop-filter: blur(10px);
      box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    
    nav ul {
      list-style: none;
      display: flex;
      justify-content: center;
      gap: 1.5rem;
    }
    
    nav ul li a {
      font-weight: 600;
      font-size: 1rem;
    }
    
    /* Hero Section */
    .hero {
      display: flex;
      flex-wrap: wrap;
      align-items: center;
      justify-content: space-between;
      padding: 3rem 0;
      background: #fff;
      border-radius: 10px;
      box-shadow: 0 4px 8px rgba(0,0,0,0.1);
      margin: 2rem 0;
    }
    
    .hero-text {
      flex: 1 1 60%;
      padding: 1rem;
    }
    
    .hero-text h1 {
      font-size: 3rem;
      margin-bottom: 1rem;
    }
    
    .hero-text p {
      font-size: 1.125rem;
      margin-bottom: 1rem;
    }
    
    .hero-img {
      flex: 1 1 35%;
      text-align: center;
      padding: 1rem;
    }
    
    .hero-img img {
      width: 300px;
      height: 300px;
      border-radius: 50%;
      object-fit: cover;
      border: 8px solid var(--primary-color);
      transition: transform 0.3s ease;
    }
    
    .hero-img img:hover {
      transform: scale(1.05);
    }
    
    /* Section Styles */
    section {
      padding: 3rem 0;
    }
    
    section:nth-of-type(even) {
      background: #fff;
    }
    
    section h2 {
      text-align: center;
      margin-bottom: 2rem;
      font-size: 2.5rem;
      position: relative;
    }
    
    section h2::after {
      content: "";
      position: absolute;
      width: 80px;
      height: 4px;
      background: var(--accent-color);
      bottom: -10px;
      left: 50%;
      transform: translateX(-50%);
      border-radius: 2px;
    }
    
    .content {
      max-width: 1000px;
      margin: auto;
    }
    
    /* Research Section */
    .research-item {
      display: grid;
      grid-template-columns: 1fr auto;
      gap: 1rem;
      align-items: center;
      padding: 1rem 0;
      border-bottom: 1px solid #ddd;
    }
    
    .research-item:last-child {
      border-bottom: none;
    }
    
    .research-item img,
    .research-item video {
      width: 150px;
      border-radius: 8px;
      transition: transform 0.3s ease;
    }
    
    .research-item img:hover,
    .research-item video:hover {
      transform: scale(1.05);
    }
    
    /* Publication Cards */
    .publication-item {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
      gap: 1rem;
      background: #fff;
      padding: 1rem;
      border-radius: 10px;
      box-shadow: 0 4px 8px rgba(0,0,0,0.05);
      margin-bottom: 2rem;
    }
    
    /* Footer */
    footer {
      background: var(--primary-color);
      color: #fff;
      text-align: center;
      padding: 1.5rem 0;
      font-size: 0.9rem;
    }
    
    footer a {
      color: #fff;
    }
    
    /* Responsive Design */
    @media (max-width: 768px) {
      .hero {
        flex-direction: column;
        text-align: center;
      }
      .hero-text, .hero-img {
        flex: 1 1 100%;
      }
      nav ul {
        flex-direction: column;
        gap: 1rem;
      }
      .research-item {
        grid-template-columns: 1fr;
        text-align: center;
      }
    }
  </style>
</head>
<body>
  <!-- Header and Navigation -->
  <header>
    <div class="container">
      <nav>
        <ul>
          <li><a href="#about">About</a></li>
          <li><a href="#news">News</a></li>
          <li><a href="#research">Research</a></li>
          <li><a href="#teaching">Teaching</a></li>
          <li><a href="#publications">Publications</a></li>
          <li><a href="#professional">Professional</a></li>
        </ul>
      </nav>
    </div>
  </header>

  <!-- Hero / About Section -->
  <section id="about" class="hero">
    <div class="container hero-content">
      <div class="hero-text">
        <h1>Viswanatha Reddy</h1>
        <p>
          I'm an SDE at Amazon and passionate about deep learning, LLMs and computer vision.
          I earned my master's degree from 
          <a href="https://www.wisc.edu/" target="_blank">University of Wisconsin-Madison</a>, where I specialized in computer vision and its applications in healthcare. During my master's, I had the privilege of collaborating with 
          <a href="https://www.biostat.wisc.edu/~yli/" target="_blank">Prof. Yin Li's</a> team.
        </p>
        <p>
          I've contributed to research at Suspect Technologies as a ML Researcher and completed internships at Amazon and Aganitha Cognitive Solutions.
        </p>
        <p>
          <a href="https://www.linkedin.com/in/viswa98/" target="_blank">LinkedIn</a> &nbsp;|&nbsp;
          <a href="mailto:viswanatha.g15@iiits.in">Email</a> &nbsp;|&nbsp;
          <a href="data/resume.pdf" target="_blank">CV</a> &nbsp;|&nbsp;
          <a href="https://scholar.google.com/citations?user=9C5TEbkAAAAJ&hl=en" target="_blank">Scholar</a> &nbsp;|&nbsp;
          <a href="https://github.com/ViswanathaReddyGajjala" target="_blank">Github</a>
        </p>
      </div>
      <!-- <div class="hero-img">
        <img src="images/empty_profile.jpg" alt="Profile Photo">
      </div> -->
    </div>
  </section>

  <!-- News Section -->
  <section id="news">
    <div class="container content">
      <h2>News</h2>
      <ul>
        <li>June 2025: Our paper <em>Agentic Prompt Optimization for Evidence-Grounded Clinical Question Answering</em> accepted as an Oral presentation at BioNLP @ ACL 2025. </li>
        <li>May 2025: Our team was awarded <em>2nd position</em> in the Shared Task on grounded question answering (QA) from electronic health records (EHRs) (ArchEHR-QA @ ACL 2025).</li>
        <li>May 2025: Paper accepted to ICML-25: “LETS Forecast: Learning Embedology for Time Series Forecasting”</li>
        <li>Nov 2024: Our paper "RICA2: Rubric-Informed, Calibrated Assessment of Actions" won the Best Poster Award at the NSF Poster Competition at Purdue University, West Lafayette, IN </li>
        <li>July 2024: Our paper <a href="https://arxiv.org/abs/2408.02138" target="_blank">RICA<sup>2</sup>: Rubric-Informed, Calibrated Assessment of Actions</a> accepted to ECCV-2024.</li>
        <li>May 2024: Invited talk on Action Quality Assessment at Google Research (Bangalore).</li>
      </ul>
    </div>
  </section>

  <!-- Research Section -->
  <section id="research">
    <div class="container content">
      <h2>Research</h2>
      <p>
        My primary focus is on computer vision and deep learning, with projects spanning Generative AI, RAG-based workflows, and custom chatbots. I also work on object detection, segmentation, incremental learning, biometrics, and video machine learning. Recent projects include:
      </p>
      <div class="research-item">
        <div>
          <a href="https://abrarmajeedi.github.io/deep_edm/" target="_blank">
            <h3>LETS Forecast: Learning Embedology for Time Series Forecasting</h3>
          </a>
          <p class="authors">
            <strong>Authors:</strong> Abrar Majeedi, Viswanatha Reddy Gajjala, Satya Sai Srinath Namburi, Nada Elkordi,Yin Li.
          </p>
          <p><em>International Conference on Machine Learning (ICML) 2025</em></p>
          <p>
            A novel time series forecasting method that combines principles from nonlinear dynamical systems with deep learning to model latent temporal structure for accurate forecasts.
          </p>
          <p>
            <a href="https://abrarmajeedi.github.io/deep_edm/" target="_blank">Project Page</a> |
            <a href="https://github.com/abrarmajeedi/DeepEDM" target="_blank">Code</a> |
            <a href="https://arxiv.org/abs/2506.06454" target="_blank">arXiv</a>
          </p>
        </div>
        <div>
          <img src="images/taken.gif" alt="Takens Theorem GIF">
        </div>
      </div>

      <div class="research-item">
        <div>
          <a href="https://abrarmajeedi.github.io/rica2_aqa/" target="_blank">
            <h3>RICA<sup>2</sup>: Rubric-Informed, Calibrated Assessment of Actions</h3>
          </a>
          <p class="authors">
            <strong>Authors:</strong> Abrar Majeedi, Viswanatha Reddy Gajjala, Satya Sai Srinath Namburi GNVV, Yin Li.
          </p>
          <p><em>ECCV 2024</em></p>
          <p>
            Incorporates human-designed scoring rubrics for calibrated uncertainty in action quality assessment.
          </p>
          <p>
            <a href="https://abrarmajeedi.github.io/rica2_aqa/" target="_blank">Project Page</a> |
            <a href="https://arxiv.org/abs/2408.02138" target="_blank">arXiv</a>
          </p>
        </div>
        <div>
          <img src="images/rica2_icon.png" alt="RICA2 Icon">
        </div>
      </div>
      <div class="research-item">
        <div>
          <a href="https://epic-kitchens.github.io/2022#results" target="_blank">
            <h3>Detecting Egocentric Actions with ActionFormer</h3>
          </a>
          <p class="authors">
            <strong>Authors:</strong> Chenlin Zhang, Lin Sui, Abrar Majeedi, Viswanatha Reddy Gajjala, Yin Li.
          </p>
          <p><em>CVPR Workshop 2022</em></p>
          <p>
            State-of-the-art approach to egocentric action detection.
          </p>
          <p>
            <a href="https://epic-kitchens.github.io/Reports/EPIC-KITCHENS-Challenges-2022-Report.pdf" target="_blank">Report</a> |
            <a href="https://github.com/happyharrycn/actionformer_release" target="_blank">Code</a>
          </p>
        </div>
        <div>
          <video id="epic_video" width="150" muted playsinline>
            <source src="images/epic_2022_video.webm" type="video/webm">
            Your browser does not support the video tag.
          </video>
        </div>
      </div>
      <div class="research-item">
        <div>
          <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Bhunia_Doodle_It_Yourself_Class_Incremental_Learning_by_Drawing_a_Few_CVPR_2022_paper.pdf" target="_blank">
            <h3>Doodle It Yourself: Class Incremental Learning by Drawing a Few Sketches</h3>
          </a>
          <p class="authors">
            <strong>Authors:</strong> Ayan Kumar Bhunia, Viswanatha Reddy Gajjala, Subhadeep Koley, Rohit Kundu, Aneeshan Sain, Tao Xiang, Yi-Zhe Song.
          </p>
          <p><em>CVPR 2022</em></p>
          <p>
            Few-shot incremental learning empowered by user-drawn sketches.
          </p>
          <p>
            <a href="https://github.com/AyanKumarBhunia/DIY-FSCIL" target="_blank">Code</a> |
            <a href="https://arxiv.org/pdf/2203.14843.pdf" target="_blank">arXiv</a> |
            <a href="https://www.marktechpost.com/2022/04/14/doodle-it-yourself-diy-fscil-a-novel-ai-framework-for-few-shot-class-incremental-learning-without-violating-the-data-privacy-and-ethical-norms/" target="_blank">Blog</a>
          </p>
        </div>
        <div>
          <img src="images/doodle_cvpr22.JPG" alt="CVPR22 Doodle">
        </div>
      </div>
    </div>
  </section>

  <!-- Teaching Experience Section -->
  <section id="teaching">
    <div class="container content">
      <h2>Teaching Experience</h2>
      <ul>
        <li>Graduate Teaching Assistant, MATH222, UW Madison, Spring 2023 (<span style="color: red;">Superior</span> rating)</li>
        <li>Graduate Teaching Assistant, MATH222, UW Madison, Fall 2022 (<span style="color: red;">Superior</span> rating)</li>
        <li>Graduate Teaching Assistant, MATH211, UW Madison, Spring 2022</li>
        <li>Graduate Teaching Assistant, MATH221, UW Madison, Fall 2021</li>
        <li>Undergraduate Teaching Assistant, IIIT Sri City, Computer Programming, Spring 2019</li>
        <li>Undergraduate Teaching Assistant, IIIT Sri City, Digital Signal Processing, Fall 2018</li>
        <li>Undergraduate Teaching Assistant, IIIT Sri City, Basic Electronic Circuits, Spring 2018</li>
        <li>Undergraduate Teaching Assistant, IIIT Sri City, Digital Signal Analysis and Applications, Fall 2017</li>
      </ul>
    </div>
  </section>

  <!-- Other Publications Section -->
  <section id="publications">
    <div class="container content">
      <h2>Other Publications</h2>
      <ul>
        <li>Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Viswanatha Reddy Gajjala, Aneeshan Sain, Tao Xiang, Yi-Zhe Song. “Partially Does It: Towards Scene-Level FG-SBIR with Partial Input.” CVPR 2022.</li>
        <li>Viswanatha Reddy Gajjala, Sai Prasanna Teja Reddy, Snehasis Mukherjee, Shiv Ram Dubey. “MERANet: Facial Micro-Expression Recognition using 3D Residual Attention Network.” ICVGIP 2021, IIT Jodhpur, ACM. <span style="color: red;">Oral</span></li>
        <li>Debapriya Tula, MS Shreyas, Viswanatha Reddy, Pranjal Sahu, Sumanth Doddapaneni, Prathyush Potluri, Rohan Sukumaran, Parth Patwa. “Offence detection in dravidian languages using code-mixing index-based focal loss.” SN COMPUT. SCI. 3, 330 (2022). <a href="https://doi.org/10.1007/s42979-022-01190-1" target="_blank">DOI</a></li>
        <li>G. Viswanatha Reddy, BSNV Chaitanya, P. Prathyush, M. Sumanth, C. Mrinalini, P. Dileep Kumar, Snehasis Mukherjee. “DFW-PP: Dynamic Feature Weighting-based Popularity Prediction for Social Media Content.” Journal of Supercomputing, Springer, Vol. 80, pp. 5708-5730, 2024.</li>
        <li>Parth Patwa, Viswanatha Reddy, Rohan Sukumaran, Sethuraman TV, Eptehal Nashnoush, Sheshank Shankar, Rishemjit Kaur, Abhishek Singh, Ramesh Raskar. “Can Self Reported Symptoms Predict Daily COVID-19 Cases?” AI4SG Workshop at IJCAI '21. <span style="color: red;">Oral</span></li>
        <li>G. Viswanatha Reddy, Snehasis Mukherjee, Mainak Thakur. “Measuring Photography Aesthetics with Deep CNNs.” IET Image Processing, Vol. 14, No. 8, pp. 1561-1570, 2020.</li>
        <li>G. Viswanatha Reddy, CVR Dharma Savarni, Snehasis Mukherjee. “Facial Expression Recognition in the Wild, by Fusion of Deep Learnt and Hand-crafted Features.” Cognitive Systems Research, Elsevier, Vol. 62, pp. 23-34, 2020.</li>
        <li>Md Shariq Suhail, Gajjala Viswanatha Reddy, G Rambabu, CVR Dharma Savarni, VK Mittal. “Multi-functional Secured Smart Home.” ICACCI 2016.</li>
      </ul>
    </div>
  </section>

  <!-- Professional & Voluntary Work Section -->
  <section id="professional">
    <div class="container content">
      <h2>Professional &amp; Voluntary Work</h2>
      <p>Served as a reviewer for:</p>
      <ul>
        <li><a href="https://www.sciencedirect.com/journal/pattern-recognition" target="_blank">Pattern Recognition</a> Journal</li>
        <li>WACV-2021, 2023, 2025; IEEE/CVF Winter Conference on Applications of Computer Vision</li>
        <li><a href="https://iclr.cc/Conferences/2024/CallForTinyPapers" target="_blank">ICLR 2024 TinyPapers</a></li>
        <li>AAAI 2024. Member of the Program Chair (DeFactify24)</li>
        <li><a href="https://www.cv4animals.com/" target="_blank">CV4Animals</a></li>
        <li>Amazon Conference on Computer Vision 2024</li>
        <li>27th International Conference on Pattern Recognition (ICPR 2024)</li>
        <li>Invitations: CVPR 2022, CVPR 2023, WACV 2022, WACV 2024.</li>
      </ul>
    </div>
  </section>

  <!-- Footer -->
  <footer>
    <div class="container">
      <p>
        <a href="https://hits.seeyoufarm.com" target="_blank">
          <img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fviswanathareddygajjala.github.io&count_bg=%233D89C8&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false" alt="Hits">
        </a>
        <a href="https://badges.toozhao.com/stats/01JT65AK6N1ZASZ50HNG0Y0QKY" target="_blank" title="Get your own page views count badge on badges.toozhao.com">
          <img src="https://badges.toozhao.com/badges/01JT65AK6N1ZASZ50HNG0Y0QKY/green.svg" alt="Page Views Count">
        </a>
      </p>
    </div>
  </footer>
  
  <!-- JavaScript for Video Hover Behavior -->
  <script>
    const epicVideo = document.getElementById('epic_video');
    if (epicVideo) {
      epicVideo.addEventListener('mouseover', function () {
        this.play();
      });
      epicVideo.addEventListener('mouseout', function () {
        this.pause();
        this.currentTime = 0;
      });
    }
  </script>
</body>
</html>
